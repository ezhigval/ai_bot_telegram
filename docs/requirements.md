## Требования к боту

### Общие

- **Полностью бесплатный**: не тратим деньги на запуск и использование.
- **Локальный контроль**: код и данные хранятся локально и в GitHub-репозитории.
- **Расширяемость**: возможность добавлять новые навыки и интеграции без переписывания ядра.

### Каналы взаимодействия

- **Mac‑клиент**:
  - нативное приложение под macOS (Swift/SwiftUI);
  - вход по `user_name` (и, при необходимости, дополнительным секретам);
  - работа с тем же пользователем и контекстом, что и в Telegram.
- **Telegram‑бот**:
  - текущий бот остаётся основным интерфейсом для быстрых взаимодействий;
  - все сообщения проксируются через единое ядро агента.

### Персонализация и память

- У пользователя есть **единый профиль**, объединяющий:
  - Telegram‑аккаунт (ID, username);
  - `user_name` из Mac‑клиента;
  - служебный внутренний идентификатор.
- Память и поведение агента зависят от профиля:
  - язык и тон общения (неформальный/формальный, уровень «дружбы»);
  - предпочтения по формату ответов (коротко/подробно, текст/голос);
  - часто используемые команды и сценарии («любимые навыки»).

### Функции бота

- **Диалоговый агент**:
  - Поддержка текстовых сообщений.
  - Поддержка голосовых сообщений и ответов голосом (если Telegram API позволяет для ботов).
  - Обучаемость на истории диалогов (локальное хранение контекста и навыков).

- **Мультимедиа и файлы**:
  - Обработка изображений.
  - Обработка видео, в том числе «кружочков».
  - Обработка голосовых сообщений.
  - Приём и базовый анализ файлов: документы, таблицы, PDF и т.д.

- **Поиск и анализ**:
  - Поиск информации в интернете (только через **бесплатные** открытые API или локальные инструменты).
  - Анализ текста и структурированных данных.

### Нефункциональные требования

- **Язык реализации**:
  - ядро ИИ и Telegram-бот — на **Python**;
  - macOS‑клиент — на **Swift/SwiftUI**;
  - старый Go‑код может использоваться как референс, но не развивается.
- **Секреты**:
  - Все токены и ключи храним только в `.env`, **никогда** не коммитим.
  - SSH-ключи/репозитории — вне репозитория, в системе или в `.env` как пути.
- **Запуск**:
  - На старте — только локальный запуск.
  - Позже — перенос на бесплатный, но достаточно мощный сервер.

### LLM и вычислительные ресурсы

- **Первый этап (строго без денег)**:
  - локальный запуск LLM через Ollama или llama.cpp;
  - модель уровня 7–8B (например, семейство Llama 3.x), оптимизированная под диалог и код;
  - возможность запуска на личном Mac без внешнего GPU.
- **Второй этап (опционально)**:
  - подключение мультимодальных локальных моделей (анализ изображений/видео);
  - при необходимости — точечное использование бесплатных облачных API в пределах фритиров.
- **Третий этап (при согласии на расходы)**:
  - поддержка платных облачных LLM как альтернативного back-end’а;
  - прозрачное переключение через конфиг/переменные окружения без переделки клиентов.



