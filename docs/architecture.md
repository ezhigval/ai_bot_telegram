## Архитектура проекта

### Верхнеуровневая схема

- **Подпроекты в одном репозитории**:
  - `core/` — Python‑ядро ИИ (FastAPI/аналог, LLM, память, инструменты).
  - `tg_bot/` — Python‑Telegram‑бот (aiogram/аналог), тонкий адаптер к `core/`.
  - `mac_client/` — нативный macOS‑клиент (Swift/SwiftUI, Xcode‑проект и дизайн‑доки).
  - `docs/` — вся документация: требования, архитектура, roadmap, промпты, задачи, процессы.
  - существующий Go‑код (`cmd/`, `internal/`) — **legacy‑референс**, который можно читать, но не развивать.
- **Клиенты**:
  - **Mac‑клиент** (нативное приложение): UI для чата, истории, работы с файлами/медиа, вход по `user_name` и синхронизация с ядром.
  - **Telegram‑бот**: чат-оболочка, которая проксирует все запросы пользователя в ядро агента.
- **Ядро агента (`core/`, Python)**:
  - HTTP/WebSocket‑сервис, поднимаемый, например, на FastAPI;
  - предоставляет стабильное API (`/v1/messages`, `/v1/history`, `/v1/profile`), к которому обращаются и Telegram‑бот, и Mac‑клиент;
  - изначально разворачивается локально, позже — может быть вынесен на внешний сервер **без изменений клиентов** (только смена `CORE_BASE_URL` в конфиге).
- **Интеграция с Telegram (`tg_bot/`)**:
  - Python‑бот на aiogram:
    - подключение к Telegram Bot API;
    - получение апдейтов (лонг-поллинг или вебхуки);
    - маршрутизация событий (текст, голос, файлы, медиа);
    - адаптер между Telegram и ядром (преобразование апдейтов в запросы к `core/`).
- **Хранение и память (`core/`)**:
  - локальное хранение диалогов и пользовательских данных (SQLite/jsonl + опциональный векторный индекс);
  - выделение уровней: «сырые диалоги» (лог) и «сжатое знание» (конспекты/skills).
- **Инструменты (tools, `core/`)**:
  - веб‑поиск (через бесплатные API и/или локальные утилиты);
  - обработка изображений/видео/аудио (через локальные библиотеки и модели);
  - работа с файлами (PDF, документы, таблицы);
  - всё это оформлено как плагины, которые может вызывать агент.

### Каркас ИИ-агента (первый шаг)

Идея — сделать **плагинную систему**, где:

- **`LLMClient`** — интерфейс для любого движка (локальная модель, внешнее бесплатное API и т.д.).
- **`Memory`** — интерфейс для работы с контекстом и историей диалогов.
- **`Tool`** — интерфейс для инструментов (веб-поиск, анализ файлов, мультимедиа).

Это позволит:

- начинать с простого/условно-бесплатного решения (например, локальная модель или free-tier API);
- позже безболезненно заменить движок на более мощный, не меняя остальной код.

### Обучение на взаимодействиях

- Логируем все диалоги локально:
  - пользовательский ID (анонимизированный, если потребуется),
  - запросы/ответы,
  - метаданные (тип сообщения: текст, голос, файл и т.д.).
- На основе истории:
  - строим «навыки» (часто повторяющиеся паттерны запросов/ответов);
  - даём агенту доступ к этой памяти при генерации ответов.

Технически это будет реализовано в виде:

- отдельного слоя `Memory` (например, SQLite + простые векторные индексы);
- фоновых задач для переработки истории в более компактные знания.

### Персональный профиль пользователя и кросс‑канальная синхронизация

- Каждый пользователь имеет **единый профиль**:
  - `user_name` из Mac‑клиента;
  - Telegram `user_id`/`username`;
  - дополнительные идентификаторы (email, системный UUID).
- Внутри ядра заводится абстракция «пользователь» с собственным пространством памяти:
  - личная история диалогов;
  - персональные настройки (тон, стиль общения, любимые инструменты);
  - сохранённые файлы/контент, к которому агент может возвращаться.
- Сообщения из всех каналов (Mac, Telegram) нормализуются в единый формат `agent.Input` и связываются с одним и тем же пользовательским профилем:
  - это даёт **единый контекст** и эффект «я общаюсь с одним и тем же ИИ», независимо от интерфейса.

### Мак‑клиент и взаимодействие с ядром

- Mac‑клиент не содержит собственной бизнес‑логики ИИ, он:
  - отвечает за UX: удобный чат, предпросмотр медиа, управление файлами;
  - аутентифицирует пользователя по `user_name` (и, при необходимости, дополнительным токенам/ключам);
  - общается с ядром по локальному HTTP/WebSocket API.
- Ядро предоставляет минимальный API:
  - `POST /v1/messages` — отправить сообщение/файл/медиа от пользователя и получить ответ;
  - `GET /v1/history` — получить историю диалога;
  - `GET /v1/profile` / `PATCH /v1/profile` — получить/обновить персональные настройки.

### Выбор LLM и вычислительные ресурсы (первоначальный этап)

- Стартовая точка — **локальные модели через Ollama/llama.cpp**:
  - базовая текстовая модель (например, Llama 3.x 8B или аналогичный 7–8B);
  - при необходимости — лёгкая мультимодальная модель (LLaVA/облегчённые vision‑модели) для картинок.
- Причины:
  - соответствие требованию «полностью бесплатный»;
  - отсутствие зависимости от внешних платных API;
  - максимальный контроль над данными (всё остаётся на твоём железе).
- Архитектура ядра допускает, что позже:
  - `LLMClient` может быть заменён на облачные модели (OpenAI, Anthropic, другие) без переписывания клиента/бота;
  - могут появиться отдельные LLM для кода, для диалога, для мультимодальности — всё это будут разные реализации интерфейса `LLMClient`.
